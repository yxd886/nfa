\section{Background and Related Work}
\label{sec:relatedwork}



\subsection{Network Function Virtualization}

NFV was introduced by a 2012 white paper \cite{nfv_whitepaper} by telecommunication operators that propose running virtualized network functions on commodity hardware. Since then, a broad range of NFV studies has been seen in the literature, including bridging the gap between specialized hardware and network functions \cite{hwang2015netvm, Han:EECS-2015-155, martins2014clickos, 199352}, scaling and managing NFV systems \cite{gember2012stratos, palkar2015e2}, flow migration among different NF instances \cite{rajagopalan2013split, khalid2016paving, gember2015opennf}, NF replication \cite{rajagopalan2013pico, sherry2015rollback}, and traffic steering \cite{simplifying}. %None of these systems provide a uniform runtime platform to execute network functions.
In these systems, the NF instances are created as software modules running on standard VMs or containers. \nfactor~customizes a uniform runtime platform to run network functions, which enables transparent resilience support for all network functions/service chains in the runtimes. In addition, a dedicated service chain instance is provisioned for each flow, enabled by the actor framework, achieving failure tolerance and high packet processing throughput with ease. Even though modular design introduced by ClickOS  \cite{martins2014clickos} %\cite{kohler2000click}
 simplifies the way how NFs are constructed, advanced control functionalities, \eg, that to enable flow migration, are still not easy to be integrated in NFs following the design. %nowadays there are new demands for NFV system, which require advanced control functionality to be integrated even into the NF softwares.

%Among the advanced control functionality, flow migration and fault tolerance are definitely the two of the most important features.
\chuan{cite as many as possible the systems you mentioned in the introduction in the following paragraph, E2 \cite{palkar2015e2}, OpenBox \cite{OpenBox}, CoMb
\cite{sekar2012design}, xOMB \cite{anderson2012xomb}, Stratos
\cite{gember2012stratos}, OpenNetVM \cite{hwang2015netvm, zhang2016opennetvm}, ClickOS \cite{martins2014clickos}; discuss more of the recent work \cite{StandardAPI} and \cite{OpenBox}}
To achieve flow migration, existing work such as OpenNF \cite{gember2015opennf} and Split/Merge \cite{rajagopalan2013split} require direct modification of the core processing logic of NF software, which is tedious and difficult to achieve. In addition, existing NFV management systems \chuan{add citation} mostly rely on the SDN controllers to carry out the flow migration protocol, involving non-negligible message passing overhead that lowers packet processing speed of the system.
%Finally, the migration process is fully controlled by a  centralized SDN controller, which may not be scalable if there are many NF instances that need flow migration service.
\nfactor~overcomes these issues using a clean separation between NF processing logic and resilience support functionalities, as well as a system design based the distributed actor framework. The actors can be migrated by communicating among themselves without the coordination from a centralized controller. A fast virtual switch is designed to achieve the functionality of a dedicated SDN switch. Only 3 rounds of request-response are needed for achieving flow migration, based on the actor framework and the customized virtual switch, enabling fast flow migration and high packet processing throughput.


Flow replication usually involves check-pointing the entire process image \chuan{clarify what process image this is referring to, NF process?} and creating a replica for the created process image \cite{sherry2015rollback}. Such checkpointing, if not designed properly, may require temporary pause of an NF process, leading to flow packet losses \chuan{this claim does not appear to be rigorous: does all checkingpointing require pausing a process? double check and revise}. \nfactor~is able to checkpoint all states of a flow in a lightweight fashion without introducing large delay, due to xxx \chuan{briefly describe how \nfactor can achieve this}, enabling transparent replication of NFs and service chains.  Existing work \cite{sherry2015rollback} rely on automated tools to extract important state variables for replicating, which xxx\chuan{explain why these tools are not ideal}.

%A NFV system \cite{nfv-white-paper} typically consists of a controller and many VNF instances. Each VNF instance is a virtualized device running NF software. VNF instances are connected into service chains, implementing certain network services, \eg, access service. Packets of a network flow go through the NF instances in a service chain in order before reaching the destination.

%A VNF instance constantly polls a network interface card (NIC) for packets. Using traditional kernel network stack incurs high context switching overhead \cite{martins2014clickos} and greatly compromise the packet processing throughput. To speed things up, hypervisors usually map the memory holding packet buffers directly into the address space of the VNF instances with the help of Intel DPDK\cite{dpdk} or netmap \cite{netmap}. VNF instances then directly fetch packets from the mapped memory area, avoiding expensive context switches. Recent NFV systems \cite{palkar2015e2, Han:EECS-2015-155, sherry2015rollback, martins2014clickos, hwang2015netvm} are all built using similar techniques.


%Even though using DPDK and netmap to improve the performance of packet processing has become a new trend. Existing flow management systems are still using kernel networking stack to implement the communication channel. On contrary, NFActor completely abandons the kernel networking stack, by constructing a reliable transmission module using DPDK. Using this reliable transmission module does not incur any context switches, thereby boosting the message throughput to 6 million messages per second in our evalution.



\subsection{Actor}

The actor programming model has been used for constructing massive, distributed systems \cite{actor-wiki, akka, newell2016optimizing, AnalysisActor}. Each actor is an independent execution unit, which can be viewed as a logical thread. In the simplest form, an actor contains an internal actor state (\eg, statistic counter, \ac{number of the out-going request} \chuan{not intuitive why an actor's state can be status of peer actors; change to another example}), a mailbox for accepting incoming messages and several message handler functions. An actor can process incoming messages using its message handlers, send messages to other actors through the built-in message passing channel, and create new actors. \ac{Actors are well suited to implement state machine, that modify its internal state based on the received message, therefore facilitates distributed protocol implementation.} \chuan{explain how the actor model facilitates `distributed' protocol implementation} In an actor system, actors are asynchronous entities that can receive and send messages as if they are running \ac{in their own threads, simplifying programmability of actors and eliminating potential race conditions which may cause the program to crash.} \chuan{not clear what it implies by `running in a dedicated process' - clarify what benefits it brings}. The actors usually run on a powerful runtime system \cite{erlang, akka, caf}, \ac{which is a uniform platform to schedule actors to execute} \chuan{explain what runtime means here}, enabling them to achieve network transparency, \ac{as actors could transparently communicate with remote actors running on different runtimes as they are all running on the same runtime.}  \chuan{clarify what `network transparency' means here}. \chuan{discuss more how the actor model provides a natural and unified way for migrating/replicating actors}. \ac{An actor could launch a remote actor and communicates with the remote actor to migrate/replicate all of its internal state. The remote actor could directly substitute the identity of the original actor whenever necessary. Therefore actor model provides a natural and unified way for migrating/replicating actors.}

The actor model is a natural fit when building distributed NFV systems. We can create one actor as one flow processing unit (a NF or a service chain, while the later is our design choice in \nfactor), and map flow packet processing to actor message processing. Meanwhile, flow migration and replication functions can be implemented as message handlers on the actors. Even though there exists this natural connection between the actor model and NF flow processing functions, we are not aware of any existing work that leverages the actor model to build an NFV system. % even though there is a natural connection among actor message processing and NF flow processing.
 To the best of our knowledge, we are the first to exploit the actor model in enabling resilient NFV systems and relevant applications, as well as to demonstrate the benefits of this actor-based approach.


%The actor programming model has been widely used to construct resilient distributed software \cite{erlang, akka, Orleans, caf}.

There are several popular actor frameworks, \eg, Scala Akka \cite{akka}, Erlang \cite{erlang}, Orleans \cite{Orleans} and C++ Actor Framework \cite{caf}. These frameworks have been used to build a broad range of distributed applications. %, including on-line games and e-commerce.
 For example, Blizzard (a famous PC game producer) and Groupon/Amazon/eBay (famous e-commerce websites) all use Akka in their production environment \cite{akka}. However, none of these frameworks are optimized for building NFV systems. In our initial prototype implementation, we built \nfactor on top of the C++ Actor Framework \cite{caf}, but the performance of that prototype turned out to be non-satisfactory \chuan{describe what performance metrics you are referring to}, due mainly to xxx \chuan{give reasons}. This inspires us to create a customized actor framework for \nfactor~with  significantly improved performance.

\chuan{move the following discussion to the runtime section}
\textbf{Lightweight Execution Context.}
There has been a recent study on constructing lightweight execution context \cite{litton2016light} in the kernel. In this work, the authors construct a lightweight execution context is contructed by creating multiple memory mapping tables in the same process. Switching among different memory tables can be viewed as switching among different lightweight execution contexts. NFActor provides a similar execution context, not for kernel processes, but for network functions. Each actor inside \nfactor provides a lightweight execution context for processing a packet along a service chain. Being a lightweight context, the actors do not introduce too much overhead as we can see from the experiment results. On the other hand, packet processing is fully monitored by the execution context, thereby providing a transparent way for migrating and replicating flow states.
