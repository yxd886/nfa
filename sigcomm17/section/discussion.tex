\section{Discussion}
\label{sec:discussion}

Even though~\nfactor provides transparent resilience for stateful NFs,~\nfactor focuses on handling per-flow state. Currently,~\nfactor could not correctly handle shared states, \ie, the states shared by a bunch of flows. Even though the NF API in~\nfactor achieves a clean separation between per-flow state and NF processing logic, it can not correctly separate shared state. Therefore, migrating and replicating flows that share states with other flows may cause un-predicted errors in~\nfactor. A potential solution to this limitation is to enforce the programmer to write a handler that explicitly deals with the inconsistency during resilience operation. We leave this to our future work.

Another limitation of~\nfactor is that~\nfactor may incorrectly handle flows with packet encapsulation. \nfactor uses the flow-5-tuple to differentiate flows. However, different flows may share the same flow-5-tuple if their flow packets are encapsulated. This is a common for flows that are sent over the same VxLAN tunnel. In that case, those flows are handled by the same flow actor, resulting in incorrect flow processing. If~\nfactor knows what kind of encapsulation the input packet uses,~\nfactor could add a decapsulation function in the virtual switch to correctly extract different flows. This is also left in our future work.

To achieve transparent resilience,~\nfactor~requires NF to be rewritten a new set of API to achieve clean separation between flow state and NF core logic, making legacy NFs difficult to run on~\nfactor. However, with the development of NFV system, there is a practical need for people to create new NFs. NFs that process flows based on flow state could achieve transparent resilient if they are implemented using~\nfactor.

The detailed flow replication process is illustrated in Fig.~\ref{fig:flow-rep}. When a flow actor is created, it acquires its replication target runtime by sending a local actor message to liason actor.  %xxx\chuan{tell which entity the flow actor query, and through which messaging approach, remote message passing or RPC?}.
The liason actor sends back a local actor message containing the ID
%\chuan{what of the replication target runtime to be sent to the requesting flow actor?}
of the replica runtime in a local actor message, selected in the round-robin fashion among all available runtimes in the same cluster which run on a different physical server from where the server hosting the runtime. %\chuan{describe what if there is no other runtime in the cluster?}.
The coordinator monitors the number of available replica runtime in the cluster and launches new replica runtimes if there are no available replica runtime.  After the flow actor acquires a valid replica runtime, whenever it finishes processing an input flow packet %\chuan{what packet is this? the first flow packet?}
, it sends a replication actor message, containing the current flow states and the input packet %\chuan{do you only need to send the 5 tuples?}
, directly to the liaison actor on the replication target runtime. The liaison actor uses the flow-5-tuple contained in the input packet of the replication actor message to check whether there exists a replica flow actor on the replication target runtime. If not, it creates a new replica flow actor using the same flow-5-tuple contained in the replication message and forwards all subsequent replication messages that share the same flow-5-tuple to that replica flow actor, which saves the flow states contained in the replication message and sends the input packet out from the output port of the replica runtime.
%The liaison actor on the replication target runtime creates a replica flow actor using the same flow-5-tuple contained in the first packet as the original flow actor, in preparation to handle all the replication messages. The replica flow actor saves the flow state and sends first packet out from the output port of the replica runtime. Subsequent flow packet received by the original flow actors are %\chuan{is that the packet is sent out from the replica instead of original runtime even if the later has not failed? Why?}.
Similar with \cite{sherry2015rollback}, the receiver on the side of the output port of the replica runtime can only observe an output packet when the flow state has been replicated, which guarantees the same output-commit property as in \cite{sherry2015rollback}. %\chuan{I think only after an original runtime fails, packets will be sent out from the replica}.
