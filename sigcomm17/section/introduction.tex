\section{Introduction}


The recent paradigm of Network Function Virtualization (NFV) advocates moving
Network Functions (NFs) out of dedicated hardware middleboxes and running them as
virtualized applications on commodity servers \cite{nfv-white-paper}. With NFV, network
operators no longer need to maintain complicated and costly hardware middleboxes. Instead,
they may launch virtualized devices (virtual machines or containers) to run NFs on the fly, which
drastically reduces the cost and complexity of deploying network services, usually consisting of a sequence of NFs such as ``firewall$\rightarrow$IDS$\rightarrow$proxy'', {\em i.e.}, a service chain.

 %However, simply running NF software in virtualized environment is not enough to satisfy the stringent requirement of NFV. What network operators need is a full-fledged NFV system, that is capable of handling different kinds of NFV management tasks.
A number of NFV management systems have been designed in recent years, \eg, E2 \cite{palkar2015e2}, OpenBox \cite{OpenBox}, CoMb
\cite{sekar2012design}, xOMB \cite{anderson2012xomb}, Stratos
\cite{gember2012stratos}, OpenNetVM \cite{hwang2015netvm, zhang2016opennetvm}, ClickOS \cite{martins2014clickos}. They implement a
broad range of NF management functionalities, including %line-speed packet processing \chuan{it is not management functionality},
 dynamic NF placement, elastic NF scaling,
load balancing, etc., which facilitate network operators in operating NF service chains in virtualized environments. However, none of the existing systems enable failure
tolerance \cite{rajagopalan2013pico, sherry2015rollback} and flow migration \cite{gember2015opennf, rajagopalan2013split, khalid2016paving} capabilities simultaneously, both of which are of pivotal importance in practical NFV systems for resilience and scalability.
%\cui{add dpdk}

{\em Failure resilience is crucial for stateful NFs.}  Many NFs maintain important per-flow states \cite{EnablingNF}. Intrusion
detection systems such as Bro \cite{bro} parse different network/application
protocols, and store and update protocol-related
states for each flow to alert potential attacks. Firewalls \cite{firewall}
maintain TCP connection-related states by parsing TCP SYN/ACK/FIN packets for
each flow. Some load-balancers \cite{lvs} use a map between flow identifiers and
the server address to modify the destination address in each flow packet.
It is critical to ensure correct recovery of flow states in case of NF instance failures, such that the connections handled by the failed NF instances do not have to be reset. In practice, middlebox vendors
strongly rejected the idea of simply resetting all active connections after failure as it
disrupts users \cite{sherry2015rollback}.

{\em Flow migration is important for long-lived flows in various scaling cases.} Existing NF management systems mostly assume dispatching new flows to newly created NF instances when existing instances are overloaded, or waiting for remaining flows to finish before shutting down a mostly idle instance, which is in fact only feasible in cases of short-lived flows. In real-world Internet systems, long-lived flows are common. Web applications %such as websites and on-line games
 usually multiplex application-level requests and responses in
one TCP connection to improve performance. For example, a web browser uses one TCP connection to exchange many requests and responses with a
web server \cite{http-keep-alive}; video-streaming
\cite{ffmpeg} and file-downloading \cite{ftp} systems maintain long-lived TCP
connection for fetching a large amount of data from CDN servers. %These applications contribute to many long-lived flows.
 When NF instances handling long flows are overloaded, some flows need to be migrated to new NF instances, in order to mitigate overload of the existing ones in a timely manner \cite{gember2015opennf}; when some NF instances are handling a few dangling long flows each, it is also more resource/cost effective to migrate the flows to one NF instance while shutting the others down.
%Without flow migration, the overload of NF instances could not be mitigated in a timely manner because of long flows \cite{gember2015opennf}.


Given the importance of failure resilience and flow migration in an NFV system, why are they absent in the existing NF management systems? The reason is simple: implementing flow migration and fault
tolerance has been a challenging task on the existing NFV software architectures. To provide resilience, important NF states must be correctly extracted from the NF software for transmitting to a new NF instance, needed both for flow migration and replication (for resilience). However, a separation between NF states and core processing logic is not enforced in the state-of-the-art implementation of NF software. Especially, important NF states may be scattered across the code base of the software, making
extracting and serializing NF states a daunting task. Patch codes need to be
manually added to the source code of different NFs to extract and serialize NF
states \cite{gember2015opennf}\cite{rajagopalan2013split}. This usually requires a huge amount of manual work to add up to
thousands of lines of source code for one NF, \eg, Gember-Jacobson {\em et al.}~\cite{gember2015opennf} report
that it needs to add 3.3K LOC for Bro \cite{bro} and 7.8K LOC for Squid caching
proxy \cite{squid}.  Realizing this difficulty, Khalid {\em et al.}~\cite{khalid2016paving} use
static program analysis technique to automate this process. However, applying
static program analysis itself is a challenging task and the inaccuracy of
static program analysis may prevent some important NF states from being
correctly retrieved.

Even if NF states can be correctly acquired and NF replicas created, flows need to be redirected to the new NF instances in cases of NF load balancing and failure recovery. In the existing systems, this is usually handled by a centralized SDN controller, which initiates and coordinates the entire migration process for each flow. Aside from compromised scalability due to the centralized control, for lossless flow migration, the controller has to perform complicated migration protocols that involve multiple passes of messages among the SDN controller, switches, migration source and migration target \cite{gember2015opennf}, which adds delay to flow processing and limits packet processing throughput of the system.
%Even if NF states can be correctly acquired, transmitting
%the states among different NFs %and the NFV system controller
% requires an
%effective message passing service. The existing NF software (\eg,
%Click\cite{kohler2000click}) does not usually provide the support for a messaging channel, and programmers need to manually add this communication
%channel into the NF software. Finally, the additional codes that are patched to
%implement resilience inevitably entangle with the core processing logic of NF
%software. It may lead to serious software bugs if not handled properly.


In this paper, we propose a software framework for building resilient NFV systems, \nfactor, exploiting the actor framework for programming distributed services \cite{actor-wiki, akka, newell2016optimizing}. Our main observation is that actor provides the unique benefits for light-weight, decentralized migration of network flow states, based on which we enable highly efficient flow migration and replication. %We use actors to keep track of the flow states and migrate the actors without a centralized coordinator of a centralized controller.
\nfactor~tracks each flow's state with our high-performance flow actor, whose design transparently separates flow state from NF processing logic. \nfactor~provides service chain processing of flows using flow actors on carefully designed uniform runtime environment, and enables fast flow migration and replication without relying much on centralized control.
%\nfactor~uses actors to process flow packets at a high throughput rate. The flow states are kept by the actors, which can be migrated without a centraliezd coordinator.
%\nfactor~includes the following modules: (i) \ac{a light-weight controller for basic cluster management} (ii) \ac{runtimes that conduct service chain processing using actors} (iii) \ac{virtual switches for balancing the workload on runtimes.} \chuan{briefly introduce key modules in nfactor system}
\nfactor~achieves transparent resilience, easy scalability and high performance in network flow processing based on the following design highlights: %\chuan{improve and add design highlights in the following}

$\triangleright$ {\em Clean separation between NF processing logic and resilience support.} Unlike existing work
\cite{gember2015opennf, sherry2015rollback} that patch functionalities for failure resilience into NF software, \nfactor~provides a clean separation between important NF states and core NF processing logic in each NF using a unique API, which makes extracting, serializing and transmitting important flow states an easy task.
Based on this, the \nfactor~framework can transparently carry out flow migration and replication operations, those needed to enable failure resilience, regardless of the concrete network function to be replicated, \ie, which we refer to as {\em transparent resilience}. Using \nfactor, programmers implementing the NFs only need to focus on the core NF logic, and the framework provides the resilience support. % On the other hand, flow actors can be transparently migrated and replicated as long as they load NF modules that are written with \nfactor~API. We refer to this as transparent resilience in~\nfactor.


$\triangleright$ {\em Per-flow micro-management.} Fundamentally different from the existing systems, \nfactor~creates a micro execution context for each flow by providing a dedicated service chain on one actor for processing packets of this flow on the actor. This can be viewed as a micro (service chain) service dedicated to the flow.
 %Actors in \nfactor~are configured with a rich set of message handlers and run on uniform runtime systems, enabling direct communication between remote actors running on different runtime systems.
%\chuan{idea of the following sentence has been covered by the paragraph above; instead you should describe how this per-flow management can enable better scalability and high speed packet processing}
The micro execution context is constructed using actor framework, which has been proven to be a light-weight and scalable abstraction for building high-performance systems \cite{newell2016optimizing}. Scheduling actors to execute only incurs a small overhead, enabling \nfactor to have a high packet processing throughput. The horizontal scalability of \nfactor is also improved as actors can be scheduled to run on uniform runtime systems.
%Inside this execution context, the flow actor can actively exchange messages with other actors and transmit flow states without disturbing the normal NF processing.


%to provide dedicated service chain services for individual flows,  \chuan{briefly discuss how the design enables easy scalability and high performance in network flow processing}.

%NFs in a service chain are deployed inside the execution context of an actor, instead of being chained through different virtualized devices (\eg, virtual machines or containers). In this way, a unique actor is responsible for processing a network flow through a dedicated service chain. This unique actor fully monitors the flow processing. It can interrupt the flow processing for flow migration or fault tolerance without the need to contact the service chain.


$\triangleright$ {\em Largely decentralized implementation.} Based on decentralized message passing of the actor framework, flow migration and replication in \nfactor~are fully automated, achieved in a fully distributed fashion without continuous monitoring of a centralized controller, which distinguishes \nfactor~from the existing NFV systems \cite{gember2015opennf}. The controller in \nfactor~is only used for controlling dynamic scaling and initiating flow migration and replication,
%\chuan{describe what the controller is used for}
thus light-weighted and failure resilient as the controller does not need to maintain complicated state generated by flow migration and can be easily replicated by storing its simple state on a reliable storage system like ZooKeeper \cite{hunt2010zookeeper}.
%\chuan{clarify why the controller is failure resilient}.
In addition, \nfactor~is implemented on top of the high speed packet I/O library, DPDK \cite{dpdk}, which further improves the performance of \nfactor.

%The actor programming model only imposes a small overhead during service chain processing and flow management, improving the performance of NFActor.


Going beyond resilience, our NFActor framework also enables several interesting applications that the existing NFV systems are difficult to support, including live NF update, flow deduplication and reliable MPTCP subflow processing. These applications require individual NFs to initiate flow migration, %explicitly notify the flow actor to migrate,
which is hard to achieve (without significant overhead) in existing systems where flow migration is initiated and fully monitored by a centralized controller. In \nfactor, these applications can utilize our decentralized and fast flow migration to achieve live NF update with almost no interruption to high-speed packet processing of the NF, best flow deduplication to conserve bandwidth, and correct MPTCP subflow processing, with ease. %NFActor framework also provides live updates to NFs that process packets at the rate of millions packets per second with almost zero down time, due to the blazingly fast flow migration speed.

We implement \nfactor~on a real-world testbed and opensource the project code \cite{projectcode} %\chuan{complete the url in the bib}.
\chuan{improve the result discussion} The result shows that the performance of the runtime system is desirable. The runtimes have almost linear scalbility. The flow migration is blazingly fast. The flow replication is scalable, achieves desirable throughput and recover fast. The dynamic scaling of NFActor framework is good with flow migration. The result of the applications are good and positive.


The rest of the paper is organized as follows. \chuan{to complete} %We present background about NFV and the actor model in Sec.~\ref{sec:background} and overview our \nfactor~system in Sec.~\ref{sec:overview}. We discuss in detail the fault tolerance and flow migration design in Sec.~\ref{sec:fm} and Sec.~\ref{sec:ft}. We show the implementation and evaluation results in Sec.~\ref{sec:implementation} and Sec.~\ref{sec:experiments}, followed by related work in Sec.~\ref{sec:relatedwork}. Sec.~\ref{sec:conclusion} concludes the paper.







%Unfortunately, few existing NFV systems could achieve the goal to resiliently maintain network functions. In most of the existing NFV management systems (i.e. E2 \cite{palkar2015e2}, OpenBox \cite{bremler2015openbox}), network functions have been treated as a black box, which consume packets from ingress ports and generate output packets from egress ports. Therefore these systems only provide a per-NF based management scheme. Even though per-NF based management has been proved to be effective in dealing with dynamic scaling and NF planning, it might compromise the reliability and resilience under certain circumstances. A typical example is during the update to important NF configuration files (i.e. Firewall rule) or to the NF softwares, the NF instances usually need to be shutdown. Due to the limitation of per-NF based management, there is no effective method to prevent established network flows from being forced to shutdown due to this process. The only way towards solving this problem and creating a fully resilient NFV system is to provide efficient per-flow based management, on top of which to achieve flow migration and replication for true resilience.
%In this paper, we propose a new NFV building framework, called NFActor.

%However, several open problems have existed with per-flow based management scheme. It is hard to migrate flows lively without direct support from NF runtime system. Therefore, in per-NF based NFV systems, migrating flows by directly changing the route of the flow may cause serious packet drop and may lead to inconsistent flow states. Several existing works including OpenNF \cite{gember2015opennf} and Split/Merge \cite{rajagopalan2013split} made initial contributions to provide a runtime that supports live flow migration. However, the scalability of their approach is limited by using a centralized controller to coordinate the entire flow migration process. The centralized controller may also be a single point of failure, making their systems vulnerable to software bugs. Also, in order to use their runtime systems, people need to add non-trivial patch code to existing NF softwares, compromising the usability of their systems.

%However, with the developement of NFV, researchers found out that managing at middlebox level could not satisfy the requirement of some applications. Some applications require direct management of a single network flow. A straightforwad example is flow migration. When migrating a flow, the NFV management system must transfer the state information associated with the flow from one middlebox to another, and redirecting the flow to the new middlebox in the mean time. Another example is fault tolerance of an individual flow. The NFV management system has to replicate flow's state on a replica and recovers flow's state on a new middlebox in case of the failure of the old middlebox.

%Realizing these limitations, we propose a new NFV management system in this paper, called NFActor. NFActor framework is designed to provide transparent, scalable and efficient flow management. NFActor framework achieves this goal by creating a micro execution context, running on top of a uniform runtime system, for each flow using actor programming model. This execution context is augmented with several message handlers to achieve basic service chain processing and flow management tasks. To transparently integrate NF softwares with the execution context, we provide a new programming interface for creating new NF modules for NFActor framework, which enforces separation between the core NF processing logic and the NF state of each flow.

%Using NFActor framework, programmers could concentrate on the internal logic design of the NF. NFs written using the NFActor programming interface could be transparently integrated with the flow management tasks provided by the actor execution context. Due to the message passing and decentralized nature of actor programming model, the flow management tasks are fully automated by actor scheduling and message passing. There is no need for the continuous monitoring of a centralized controller. Therefore the controller in NFActor framework is extremely light-weighted and failure resilient. The actor programming model only imposes a small overhead during service chain processing and flow management, improving the performance of NFActor.

%Besides strong resilience through per-flow management, NFActor framework also enables several interesting new applications that existing NFV systems are hard to provide. These applications utilize the feature of decentralized flow migration to reduce the output bandwidth consumption during deduplication and ensures correct MPTCP subflow processing. NFActor framework also provides live updates to NFs that process packets at the rate of millions packets per second with almost zero down time, due to the blazingly fast flow migration speed.

%We implement NFActor framework on top of DPDK and evaluate its performance. The result shows that the performance of the runtime system is desirable. The runtimes have almost linear scalbility. The flow migration is blazingly fast. The flow replication is scalable, achieves desirable throughput and recover fast. The dynamic scaling of NFActor framework is good with flow migration. The result of the applications are good and positive.
