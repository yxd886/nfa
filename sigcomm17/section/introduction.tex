\section{Introduction}


The recent paradigm of Network Function Virtualization (NFV) advocates moving
Network Functions (NFs) out of dedicated hardware middleboxes and running them as
virtualized applications on commodity servers \cite{nfv-white-paper}. With NFV, network
operators no longer need to maintain complicated and costly hardware middleboxes. Instead,
they may launch virtualized devices (virtual machines or containers) to run NFs on the fly, which
drastically reduces the cost and complexity of deploying network services, usually consisting of a sequence of NFs such as ``firewall$\rightarrow$IDS$\rightarrow$proxy'', {\em i.e.}, a service chain.

 %However, simply running NF software in virtualized environment is not enough to satisfy the stringent requirement of NFV. What network operators need is a full-fledged NFV system, that is capable of handling different kinds of NFV management tasks.
A number of NFV management systems have been designed in recent years, \eg, E2 \cite{palkar2015e2}, OpenBox \cite{bremler2015openbox}, CoMb
\cite{sekar2012design}, xOMB \cite{anderson2012xomb}, Stratos
\cite{gember2012stratos}, OpenNetVM \cite{hwang2015netvm, zhang2016opennetvm}, ClickOS \cite{martins2014clickos}. They implement a
broad range of NF management functionalities, including %line-speed packet processing \chuan{it is not management functionality},
 dynamic NF placement, elastic NF scaling,
load balancing, etc., which facilitate network operators in operating NF service chains in virtualized environments. However, none of the existing systems enable failure
tolerance \cite{rajagopalan2013pico, sherry2015rollback} and flow migration \cite{gember2015opennf, rajagopalan2013split, khalid2016paving} capabilities simultaneously, both of which are of pivotal importance in practical NFV systems for resilience and scalability.
\cui{add dpdk}

{\em Failure resilience is crucial for stateful NFs.}  Many NFs maintain important per-flow states. Intrusion
detection systems such as Bro \cite{bro} parse different network/application
protocols, and store and update protocol-related
states for each flow to alert potential attacks. Firewalls \cite{firewall}
maintain TCP connection-related states by parsing TCP SYN/ACK/FIN packets for
each flow. Some load-balancers \cite{lvs} use a map between flow identifiers and
the server address to modify the destination address in each flow packet.
It is critical to ensure correct recovery of flow states in case of NF instance failures, such that the connections handled by the failed NF instances do not have to be reset. In practice, middlebox vendors
strongly rejected the idea of simply resetting all active connections after failure as it
disrupts users \cite{sherry2015rollback}.

{\em Flow migration is important for long-lived flows in various scaling cases.} Existing NF management systems mostly assume dispatching new flows to newly created NF instances when existing instances are overloaded, or waiting for remaining flows to finish before shutting down a mostly idle instance, which is in fact only feasible in cases of short-lived flows. In real-world Internet systems, long-lived flows are common. Web applications %such as websites and on-line games
 usually multiplex application-level requests and responses in
one TCP connection to improve performance. For example, a web browser uses one TCP connection to exchange many requests and responses with a
web server \cite{http-keep-alive}; video-streaming
\cite{ffmpeg} and file-downloading \cite{ftp} systems maintain long-lived TCP
connection for fetching a large amount of data from CDN servers. %These applications contribute to many long-lived flows.
 When NF instances handling long flows are overloaded, some flows need to be migrated to new NF instances, in order to mitigate overload of the existing ones in a timely manner \cite{gember2015opennf}; when some NF instances are handling a few dangling long flows each, it is also more resource/cost effective to migrate the flows to one NF instance while shutting the others down.
%Without flow migration, the overload of NF instances could not be mitigated in a timely manner because of long flows \cite{gember2015opennf}.


Given the importance of failure resilience and flow migration in an NFV system, why are they absent in the existing NF management systems? The reason is simple: implementing flow migration and fault
tolerance has been a challenging task on the existing NFV software architectures. To provide resilience, important NF states must be correctly extracted from the NF software for transmitting to a new NF instance. However, a separation between NF states and core processing logic is not enforced in the state-of-the-art implementation of NF software. Especially, important NF states may be scattered across the code base of the software, making
extracting and serializing NF states a daunting task. Patch codes need to be
manually added to the source code of different NFs to extract and serialize NF
states \cite{gember2015opennf}\cite{rajagopalan2013split}. This usually requires a huge amount of manual work to add up to
thousands of lines of source code for one NF, \eg, \cite{gember2015opennf} reports
that it needs to add 3.3K LOC for Bro \cite{bro} and 7.8K LOC for Squid caching
proxy \cite{squid}.  Realizing this difficulty, \cite{khalid2016paving} uses
static program analysis technique to automate this process. However, applying
static program analysis itself is a challenging task and the inaccuracy of
static program analysis may prevent some important NF states from being
correctly retrieved.

\chuan{Modify this into an centralized SDN argument.}
\ac{Even if NF states can be correctly acquired, resilience operation, especially flow migration, needs to modify the flow state and redirect the flow to a new target. In existing systems, this is usually handled by a centralized SDN controller, which needs to initiate and coordinate the the entire migration process for each flow. Aside from compromised scalability due to the centralized control, to migrate losslessly migrate a flow, the controller has to perform complicated migration protocols that involves passing multiple message among SDN switches, migration source and migration target \cite{gember2015opennf}, which limits the applicability of the flow migration.}
%Even if NF states can be correctly acquired, transmitting
%the states among different NFs %and the NFV system controller
% requires an
%effective message passing service. The existing NF software (\eg,
%Click\cite{kohler2000click}) does not usually provide the support for a messaging channel, and programmers need to manually add this communication
%channel into the NF software. Finally, the additional codes that are patched to
%implement resilience inevitably entangle with the core processing logic of NF
%software. It may lead to serious software bugs if not handled properly.


In this paper, we propose a software framework for building resilient NFV systems, \nfactor, exploiting the actor framework for programming distributed services \cite{actor-wiki, akka, newell2016optimizing}. Our main observation is that actor provides the unique benefits for light-weight, decentralized migration of network flow states. %We use actors to keep track of the flow states and migrate the actors without a centralized coordinator of a centralized controller.
\nfactor~tracks each flow's state with our high-performance flow actor, which transparently separates flow state from NF processing logic. \nfactor's flow migration is achieved by flow actors themselves without involving a centralized coordinator.
%\nfactor~uses actors to process flow packets at a high throughput rate. The flow states are keeped by the actors, which can be migrated without a centraliezd coordinator.

%\nfactor~includes the following modules: (i) \ac{a light-weight controller for basic cluster management} (ii) \ac{runtimes that conduct service chain processing using actors} (iii) \ac{virtual switches for balancing the workload on runtimes.} \chuan{briefly introduce key modules in nfactor system}
\nfactor~achieves transparent resilience, easy scalability and high performance in network flow processing based on the following technical highlights: \chuan{improve and add design highlights in the following}

$\triangleright$  Unlike existing work
\cite{gember2015opennf, sherry2015rollback} that patch resilience
functionalities into NF software, \nfactor~achieves {\em transparent resilience} by providing a clean separation between important NF states and core NF processing logic in each NF module using a unique API, which makes extracting, serializing and transmitting important flow states an easy task.
\ac{\nfactor~enforces a complete separation between resilience operations and NF module implementation. Using \nfactor, programmers implementing the NF module only needs to focus on the core NF logic, they do not need to take care of the underlying resilient operations. On the other hand, flow actors can be transparently migrated and replicated as long as they load NF modules that are written with \nfactor~API. We refer to this as transparent resilience in~\nfactor.}
%说我的那个个想法， 然后i.e transparent resilience.


$\triangleright$ Fundamentally different from the existing systems, \nfactor~adopts a per-flow management principle. \ac{\nfactor~manages flows through a one-actor-one-flow abstraction. Packet processing of a flow are completely delegated to a unique actor that is specially created for that flow. %Actors in \nfactor~are configured with a rich set of message handlers and run on uniform runtime systems, enabling direct communication between remote actors running on different runtime systems.
\nfactor~creates a micro execution context for each flow using flow actors. Inside this execution context, a flow actor could actively exchange messages with other actors and transmit flow states without disturbing the normal NF processing. This serves as the basis for transparent resilience. }
\chuan{describe what the per-flow management is about and its advantages}

%to provide dedicated service chain services for individual flows,  \chuan{briefly discuss how the design enables easy scalability and high performance in network flow processing}.

%NFs in a service chain are deployed inside the execution context of an actor, instead of being chained through different virtualized devices (\eg, virtual machines or containers). In this way, a unique actor is responsible for processing a network flow through a dedicated service chain. This unique actor fully monitors the flow processing. It can interrupt the flow processing for flow migration or fault tolerance without the need to contact the service chain.


$\triangleright$ Resilience support in \nfactor~is provided in a fully distributed fashion, without directly involving a central controller, which distinguishes \nfactor~from the existing NFV systems \cite{gember2015opennf}. Due to the message passing and decentralized nature of actor programming model, the flow management tasks are fully automated by actor scheduling and message passing. There is no need for the continuous monitoring of a centralized controller. Therefore the controller in NFActor framework is extremely light-weighted and failure resilient. The actor programming model only imposes a small overhead during service chain processing and flow management, improving the performance of NFActor. \ac{~\nfactor is implemented on top of high speed packet I/O library DPDK \cite{dpdk}, which further improves the performance of \nfactor.}

Going beyond resilience, NFActor framework also enables several interesting new applications that existing NFV systems are hard to provide \chuan{explain why hard}. \ac{These applications require NFs to explicitly notify the flow actor to migrate, in existing systems where flow migrations are initiated and fully monitored by a centralized coordinator, this is hard to achieve.} These applications utilize the feature of decentralized flow migration to reduce the output bandwidth consumption during deduplication and ensures correct MPTCP subflow processing. NFActor framework also provides live updates to NFs that process packets at the rate of millions packets per second with almost zero down time, due to the blazingly fast flow migration speed.

We implement \nfactor~on a real-world testbed. \chuan{improve the result discussion} The result shows that the performance of the runtime system is desirable. The runtimes have almost linear scalbility. The flow migration is blazingly fast. The flow replication is scalable, achieves desirable throughput and recover fast. The dynamic scaling of NFActor framework is good with flow migration. The result of the applications are good and positive.


The rest of the paper is organized as follows. \chuan{to complete} %We present background about NFV and the actor model in Sec.~\ref{sec:background} and overview our \nfactor~system in Sec.~\ref{sec:overview}. We discuss in detail the fault tolerance and flow migration design in Sec.~\ref{sec:fm} and Sec.~\ref{sec:ft}. We show the implementation and evaluation results in Sec.~\ref{sec:implementation} and Sec.~\ref{sec:experiments}, followed by related work in Sec.~\ref{sec:relatedwork}. Sec.~\ref{sec:conclusion} concludes the paper.







%Unfortunately, few existing NFV systems could achieve the goal to resiliently maintain network functions. In most of the existing NFV management systems (i.e. E2 \cite{palkar2015e2}, OpenBox \cite{bremler2015openbox}), network functions have been treated as a black box, which consume packets from ingress ports and generate output packets from egress ports. Therefore these systems only provide a per-NF based management scheme. Even though per-NF based management has been proved to be effective in dealing with dynamic scaling and NF planning, it might compromise the reliability and resilience under certain circumstances. A typical example is during the update to important NF configuration files (i.e. Firewall rule) or to the NF softwares, the NF instances usually need to be shutdown. Due to the limitation of per-NF based management, there is no effective method to prevent established network flows from being forced to shutdown due to this process. The only way towards solving this problem and creating a fully resilient NFV system is to provide efficient per-flow based management, on top of which to achieve flow migration and replication for true resilience.
%In this paper, we propose a new NFV building framework, called NFActor.

%However, several open problems have existed with per-flow based management scheme. It is hard to migrate flows lively without direct support from NF runtime system. Therefore, in per-NF based NFV systems, migrating flows by directly changing the route of the flow may cause serious packet drop and may lead to inconsistent flow states. Several existing works including OpenNF \cite{gember2015opennf} and Split/Merge \cite{rajagopalan2013split} made initial contributions to provide a runtime that supports live flow migration. However, the scalability of their approach is limited by using a centralized controller to coordinate the entire flow migration process. The centralized controller may also be a single point of failure, making their systems vulnerable to software bugs. Also, in order to use their runtime systems, people need to add non-trivial patch code to existing NF softwares, compromising the usability of their systems.

%However, with the developement of NFV, researchers found out that managing at middlebox level could not satisfy the requirement of some applications. Some applications require direct management of a single network flow. A straightforwad example is flow migration. When migrating a flow, the NFV management system must transfer the state information associated with the flow from one middlebox to another, and redirecting the flow to the new middlebox in the mean time. Another example is fault tolerance of an individual flow. The NFV management system has to replicate flow's state on a replica and recovers flow's state on a new middlebox in case of the failure of the old middlebox.

%Realizing these limitations, we propose a new NFV management system in this paper, called NFActor. NFActor framework is designed to provide transparent, scalable and efficient flow management. NFActor framework achieves this goal by creating a micro execution context, running on top of a uniform runtime system, for each flow using actor programming model. This execution context is augmented with several message handlers to achieve basic service chain processing and flow management tasks. To transparently integrate NF softwares with the execution context, we provide a new programming interface for creating new NF modules for NFActor framework, which enforces separation between the core NF processing logic and the NF state of each flow.

%Using NFActor framework, programmers could concentrate on the internal logic design of the NF. NFs written using the NFActor programming interface could be transparently integrated with the flow management tasks provided by the actor execution context. Due to the message passing and decentralized nature of actor programming model, the flow management tasks are fully automated by actor scheduling and message passing. There is no need for the continuous monitoring of a centralized controller. Therefore the controller in NFActor framework is extremely light-weighted and failure resilient. The actor programming model only imposes a small overhead during service chain processing and flow management, improving the performance of NFActor.

%Besides strong resilience through per-flow management, NFActor framework also enables several interesting new applications that existing NFV systems are hard to provide. These applications utilize the feature of decentralized flow migration to reduce the output bandwidth consumption during deduplication and ensures correct MPTCP subflow processing. NFActor framework also provides live updates to NFs that process packets at the rate of millions packets per second with almost zero down time, due to the blazingly fast flow migration speed.

%We implement NFActor framework on top of DPDK and evaluate its performance. The result shows that the performance of the runtime system is desirable. The runtimes have almost linear scalbility. The flow migration is blazingly fast. The flow replication is scalable, achieves desirable throughput and recover fast. The dynamic scaling of NFActor framework is good with flow migration. The result of the applications are good and positive.
