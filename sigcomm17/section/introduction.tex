\section{Introduction}

The recent paradigm of Network Function Virtualization (NFV) advocates moving Network Functions (NFs) out of dedicated hardware middleboxes and running them as virtualized applications on commodity servers \cite{nfv-white-paper}. With NFV, network operators no longer need to maintain complicated and costly hardware middleboxes. Instead, they can launch virtualized devices (virtual machines or containers) to run NFs on the fly, which drastically reduces the cost and complexity of deploying network services, usually consisting of a sequence of NFs such as ``firewall$\rightarrow$IDS$\rightarrow$proxy''.

%很长时间以来，middlebox都会被人们当成一个黑箱。网络包会被送入黑箱的入口并从黑箱的出口送出。人们通常不会直接关注网络包在middlebox内的处理情况。基于这种理念，现有的nfv 管理系统通常只会对middlebox进行直接管理，比如将不同的middlebox连接成所需要的service chain，以及将不同的middlebox部署在不同的物理服务器上，以及提供动态扩展的服务。

For a long period of time, middleboxes have been treated as a black box, which consume packets from ingress ports and generate output packets from egress ports. Usually, people do not concern on how packets are processed inside a middlebox. Based on this idea, most of the existing NFV management systems (i.e. E2 \cite{palkar2015e2}, OpenBox \cite{bremler2015openbox}, CoMb \cite{sekar2012design}, xOMB \cite{anderson2012xomb}, Stratos \cite{gember2012stratos}, OpenNetVM \cite{hwang2015netvm, zhang2016opennetvm}, ClickOS \cite{martins2014clickos}) manage at middlebox level. Taking E2 \cite{palkar2015e2} as an example, E2 builds a service graph to determine how the service chain are constructed and which physical server should a VNF instance be placed on. E2 also monitors the workload on each VNF instance to determine when to dynamically scale the system.

%但是随着NFV的发展，研究人员们发现仅仅从middlebox层面进行管理是无法满足某些应用的需求的。某些应用需要直接对单个网络流进行直接管理。一个直观的例子就是网络流迁移。当进行网络流迁移时，管理系统必须将一个流保存在middlebox内的状态提取出并传递到其他的middlebox,同时改变这个流的路由。另外一个例子是容错，当进行容错时，我们需要将流所对应的状态保存到一个replica上，并在原middlebox出错时，在一个新的middlebox上恢复这个流的状态并。

However, with the developement of NFV, researchers found out that managing at middlebox level could not satisfy the requirement of some applications. Some applications require direct management of a single network flow. A straightforwad example is flow migration. When migrating a flow, the NFV management system must transfer the state information associated with the flow from one middlebox to another, and redirecting the flow to the new middlebox in the mean time. Another example is fault tolerance of an individual flow. The NFV management system has to replicate flow's state on a replica and recovers flow's state on a new middlebox in case of the failure of the old middlebox.

%在所列出的nfv研究中，OpenNF是一个典型的可以对单个网络流进行管理的例子。虽然OpenNF为管理单个网络流开了一个好头，但是它自身还是存在一定的不足。首先，在OpenNF中，所有的流管理都是从一个中央花的sdn控制器发出的，这限制了系统的可扩展性。因为随着系统规模的扩大，这个中央化的控制器将不可避免的成为一个瓶颈。第二，OpenNF并没有提供一个统一的针对流的执行环境，它需要对已有的middlebox来打补丁，以获取它的状态并和控制器进行通信，这种方法使得adapting OpenNF变得非常困难。第三，OpenNF并没有针对高速NFV系统进行优化，它的传输环境仍然基于传统的内核socket网络。这将使它的吞吐量受到极大的限制。

There are some well known systems on managing individual network flows \cite{gember2015opennf, rajagopalan2013split, khalid2016paving}. Even though these systems pave way for the future research, they have some limitations that compromise their applicability. First of all, in these systems, the flow management tasks are initiated from a central SDN controller. This architecture limits the scalability of the system. When the number of VNF instance and the traffic volume increase, this central SDN controller inevitably becomes the bottleneck in the system. Secondly, existing systems do not provide a uniform exeuction context for managing individual flow. Additional patch codes must be added to the middlebox software when using these systems, to acquire the state associated with the flow and to communicate with the centralized controller This makes adapting these systems tedious and hard. Finally, the communication channel, which is heavily used by these systems to transmit flow states, are not optimized for high speed NFV application. It is still based on the traditional kernel networking stack, which has been proved to be a performance bottleneck \cite{martins2014clickos}, thereby limiting the maximum packet throughput these systems can achieve.

%意识到这些不足，在本文中，我们提出了一个新的NFV管理系统，叫做NFActor。NFActor提供了一个分布式的运行时环境。我们使用一个轻量级的控制器来协调所有的运行时环境。在运行时环境中，我们以actor programming model为基础为网络流制作了统一execution context。我们在这个execution context里嵌入了流迁移以及容错的消息处理函数。同时，我们提供了一个实现NF的接口，这个接口分离了NF的核心处理逻辑以及每一个流的状态。最后，我们利用DPDK的高速包IO功能制作了一个简单但有效的可靠传输系统，用来传递actor远程通信时所产生的消息。这所有的一切都被运行时内部的调度器高效的调度。

Reliazing these limitations, we propose a new NFV management system in this paper, called NFActor. NFActor provides a distributed runtime environment, which could be controlled by a light-weight controller. Inside a runtime, we use actor programming model \cite{actor-wiki} to construct a uniform execution context for each network flow. The execution context is augmented with different kinds of message handlers for managing flow migration and fault tolerance. In the mean time, we provide a new interface for programming new NFs. This interface simply separates the core NF processing logic with the state of each flow. Finally, we make a simple yet efficient reliable transmission modle using the high-speed packet I/O functionality provided by DPDK \cite{dpdk}. This reliable tranmission module is used to pass all the messages during remote actor communication. All these parts are scheduled by a simple round-rubin scheduler inside the runtime.

%在NFActor中，流迁移与容错和NF的核心处理逻辑完全解耦。在actor programming model的帮助下， 每一个流都可以自己实现迁移和容错，不需要一个中央控制器进行显示的控制。流迁移和容错的速度也非常快，符合现代NFV系统对高速处理包的需求。同时，这个flow execution context只对正常的NF处理产生很小的overhead，我们的试验结果也表明NFActor 运行时系统可以实现desirable的流吞吐量。

The result of this architecture is the complete decoupling of flow management tasks from a centralized controller. Using its own execution context, each flow could migrate or replicate itself, without the coordination from a centralized controller. Even though new NF must be written specifically for NFActor architecture, it is not considered harmful \cite{199352}. The goold news is that programmers who write new NFs for NFActor only need to concentrate on the NF logic design. Once the NF is completed, it will be spontanesouly integrated with the flow execution context. The abstraction of flow execution context only incurs a small overhead when processing packet. Our evaluation results show that NFActor could achieve desirable packet throughput. The performance of flow migration and fault tolerance is also satasfactory according to the standard of modern high-performance NFV systems.  

%A number of NFV management systems have been designed in recent years, \eg, E2 \cite{palkar2015e2}, OpenBox \cite{bremler2015openbox}, CoMb \cite{sekar2012design}, xOMB \cite{anderson2012xomb}, Stratos \cite{gember2012stratos}, OpenNetVM \cite{hwang2015netvm, zhang2016opennetvm}, ClickOS \cite{martins2014clickos}. They implement a broad range of NF management functionalities, including line-speed packet processing, dynamic NF placement, elastic NF scaling, load balancing, etc., to facilitate network operators in deploying NFs in virtualized environments. However, none of these NF management systems are guaranteed to be resilient, and they may not enable fault tolerance \cite{rajagopalan2013pico, sherry2015rollback} and flow migration \cite{gember2015opennf, rajagopalan2013split, khalid2016paving} simultaneously.

%Failure resilience with flow migration capability is of pivotal importance in practical NFV systems. Existing NF management systems mostly assume dispatching new flows to newly created NF instances when existing instances are overloaded, which is in fact only feasible in cases of short-lived flows. In real-world Internet systems, long-lived flows are common. Web applications usually multiplex application-level requests and responses in one TCP connection to improve performance. For example, a web browser commonly enables HTTP keep-alive to use one TCP connection to exchange many requests and responses with a web server \cite{http-keep-alive}. In video-streaming \cite{ffmpeg} and file-downloading \cite{ftp} system, long-lived TCP connection are also often maintained for fetching a large amount of data from servers. When NF instances handling long flows are overloaded, some flows need to be migrated to new NFs, in order to mitigate overload of the existing ones in a timely manner \cite{gember2015opennf}.

%On the other hand, many NFs maintain important per-flow states. Intrusion detection systems such as Bro \cite{bro} parse different network/application protocols, and store and update protocol-related states for each flow to alert potential attacks. Firewalls \cite{firewall} maintain TCP connection-related states by parsing TCP SYN/ACK/FIN packets for each flow. Some load-balancers \cite{lvs} use a map between flow identifiers and the server address to modify the destination address in each flow packet. It is critical to ensure correct recovery of flow states in case of NF instance failures, such that the connections handled by the failed NF instances do not have to be reset. In practice, middlebox vendors strongly rejected the idea of simply resetting all active connections after failure as it disrupts users \cite{sherry2015rollback}.

%Given the importance of failure resilience and flow migration in an NFV system, why are they absent in the existing NF management systems? The reason is simple: implementing flow migration and fault tolerance has been a challenging task on the existing NFV software architectures. To provide resilience, important NF states must be correctly extracted from the NF software for transmitting to a new NF instance. However, a separation between NF states and core processing logic is not enforced in the state-of-the-art implementation of NF software. Especially, important NF states may be scattered across the code base of the software, making extracting and serializing NF states a daunting task. Patch codes need to be manually added to the source code of different NFs to extract and serialize NF states \cite{gember2015opennf}\cite{rajagopalan2013split}. This usually requires a huge amount of manual work to add up to thousands of lines of source code for one NF, \eg, \cite{gember2015opennf} reports that it needs to add 3.3K LOC for Bro \cite{bro} and 7.8K LOC for Squid caching proxy \cite{squid}.  Realizing this difficulty, \cite{khalid2016paving} uses static program analysis technique to automate this process. However, applying static program analysis itself is a challenging task and the inaccuracy of static program analysis may prevent some important NF states from being correctly retrieved.

%Even if NF states can be correctly acquired, transmitting the states among different NFs requires an effective message passing service. The existing NF software (\eg, Click\cite{kohler2000click}) does not usually provide the support for a messaging channel, and programmers have to manually add this communication channel into the NF software. Finally, the additional codes that are patched to implement resilience inevitably entangle with the core processing logic of NF software. It may lead to serious software bugs if not handled properly.

%In this paper, we propose a software framework for building resilient NFV systems, \nfactor, exploiting the actor framework for programming distributed services \cite{actor-wiki, akka, newell2016optimizing} Unlike existing work \cite{gember2015opennf, sherry2015rollback} that patch resilience functionalities into NF software, \nfactor~is an NFV system with transparent resilience: (i) based on the actor programming model, a clean separation between important NF states and core NF processing logic is enabled in each NF module by a unique API, which makes extracting, serializing and transmitting important flow states an easy task; (ii) a new service chain abstraction enables running NF software modules belonging to the same service chain inside the execution context of one actor, and the same runtime program is running on all containers containing multiple actors in the system ; (iii) a built-in efficient communication channel across the actors is available, in the actor framework.


%Our detailed contributions in designing \nfactor~can be summarized as follows.

%{\em First}, we introduce the actor programming model into NFV systems. NFV systems built on top of this model achieve transparent resilience and can exploit the efficient communication channel provided in the actor framework. Using \nfactor, programmer implementing NF modules only needs to focus on the core processing logic. The \nfactor~framework automatically handles fault tolerance and flow migration for the created NF modules. What's more, resilience support in \nfactor~is provided in a fully distributed fashion, without directly involving a central controller, which distinguishes \nfactor~from the existing NFV systems \cite{gember2015opennf}.

%{\em Second}, we design an API for implementing each NF module, which enables a clean separation between important NF states and core NF processing logic. We propose a new service chain abstraction, where NFs in a service chain are deployed inside the execution context of an actor, instead of being chained through different virtualized devices (\eg, virtual machines or containers). In this way, a unique actor is responsible for processing a network flow through a dedicated service chain. This unique actor fully monitors the flow processing. It can interrupt the flow processing for flow migration or fault tolerance without the need to contact the service chain. We run the same runtime program on all containers containing multiple actors. The use of the container incurs only a small overhead and thereby improve the packet processing performance of NFActor framework. And using the same program ensures that a uniform execution environment is provided for all the actors, which facilitate the design of flow migration and fault tolerance.

%{\em Third}, we design a novel distributed flow migration method and a lightweight %flow state replication method, enabling fast and lightweight flow migration and failure recovery. The flow migration protocol used by NFActor framework only involves the transmission of 3 request-responses. Evaluation result shows that the migration of a single flow in an environment with small workload could be completed within 700us. The use of the actor abstraction enables NFActor framework to independently replicate the state of a single flow, thereby eliminating the need to halt the execution of the entire program.

%To enable a NF in \nfactor, core processing logic of an NF needs to be implemented following the actor programming framework. Nevertheless, porting the core processing logic of an existing NF software should be relatively straightforward since the APIs provided by our \nfactor~framework is extremely simple. The evaluation result shows that NFActor only incurs acceptable overhead when running NF modules. And NFActor runtime systems have a promising linear scalability. For the resilience functionality, NFActor framework out-performs existing flow migration system by more than 50\% in terms of migration completion time and NFActor framework achieves consistent recovery time under varying workload.

%The rest of the paper is organized as follows. We present background about NFV and the actor model in Sec.~\ref{sec:background} and overview our \nfactor~system in Sec.~\ref{sec:overview}. We discuss in detail the fault tolerance and flow migration design in Sec.~\ref{sec:fm} and Sec.~\ref{sec:ft}. We show the implementation and evaluation results in Sec.~\ref{sec:implementation} and Sec.~\ref{sec:experiments}, followed by related work in Sec.~\ref{sec:relatedwork}. Sec.~\ref{sec:conclusion} concludes the paper.
