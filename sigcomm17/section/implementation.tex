\section{Implementation}
\label{sec:implementation}

%我们使用C++来实现NFActor。在当前的实现中，NFActor的每一个runtime都跑在一个container中，不同runtime通过BESS进行连接。每一个runtime的端口都是一个BESS的zerocopy vport。

%重用bess module system。
%NFActor runtime的内部架构复用了BESS的module系统以及bess的调度器。我们将bess module system和scheduler代码port到了nfactor runtime的代码里。并且用

NFActor framework is implemented in C++. The core functionality of NFActor framework contains around 8500 lines of code. We use BESS \cite{bess}\cite{2015} as the dataplane inter-connection tool to connect different runtimes and virtual switches. The three ports that are assigned to each runtime are zero-copy VPort in BESS, which is a high-speed virtual port for transmitting raw packets. BESS could build a virtual L2 ethernet inside a server and connect this virtual ethernet to the physical L2 ethernet. By connecting the virtual L2 ethernet with the ports of runtimes, We can connect different runtimes running on different servers together.

\subsection {Reuse BEES Module System}

The runtime needs to poll packets from the input port, schedule flow actors to run and transmit remote actor messages. To schedule these tasks efficiently, we decide to reuse BESS module systems. BESS module system is specifically designed to schedule packet processing pipelines in high-speed NFV systems, which is a perfect suit to NFActor runtime architecture. We port the BESS module system and BESS module scheduler to the runtime and implement all the actor processing tasks as BESS modules. These modules are connected into the following 5 pipelines.

\begin{itemize}

\item The first/second pipeline polls packets from the input/output port, runs actor scheduler on these packets and sends the packets out from the output/input port.

\item The third pipeline polls packets from control ports, reconstruct packet stream into remote actor messages and send the actor messages to the receiver actors. (The first/second pipeline also carries out this processing because remote messages are also sent to input/output port \ref{}).

\item The fourth pipeline schedules coordinator actor to execute RPC requests sent from the controller. In particular, coordinator actor updates the configuration information of other runtimes in the cluster and dispatches flow migration initiation messages to active flow actors in the runtime.

\item When processing the previous four pipelines, the actors may send remote actor messages. These messages are placed into ring buffers \ref{}. The fifth pipeline fetches remote actor messages from these ring buffers and sends remote actor messages out from corresponding ports.

\end{itemize}

The runtime uses BESS scheduler to schedule these 5 pipelines in a round-rubin manner to simulate a time-sharing scheduling.

\subsection{Customized Actor Library}
\label{sec:actor-library}

To minimize the overhead of actor programming, we choose to implement our own actor library. In this actor library, due to the single-worker-thread design, local actor message transmission is directly implemented as a function call, therefore eliminating the overhead of enqueuing and dequeuing messages from an actor mailbox \cite{}. For remote actor message passing, we assign a unique ID to each runtime and each actor. The sender actor only needs to specify the receiver actor's ID and runtime ID, then the reliable transmission module \ref{} could deliver the remote actor message to the receiver actor.

To schedule flow actors, we directly run a flow actor scheduler in the first three pipelines. The flow actor scheduler redirects both input flow packets and remote actor messages to corresponding flow actors, by looking up the flow actors using the flow identifier.

Even though the functionality implemented by our customized actor library is very simple compared with other mature actor libraries \cite{akka} \cite{caf}, the simple architecture of our actor library decreases overhead associated with actor processing, enabling~\nfactor to satisfy the high-speed packet processing requirement of mordern NFV system. %Our customized actor library could not achieve complicated ac as other mature actor frameworks do \cite{akka} \cite{caf}. However, due to its simple architecture, it only imposes a small overhead when doing actor processing, therefore it is able to satisfy the high-speed packet processing requirement of modern NFV system.

\subsection{Reliable Message Passing Module}

To reliably deliver remote actor messages, we build a customized reliable message passing module for NFActor framework. Unlike user-level TCP stack, where messages are inserted into a reliable byte stream and transmitted to the other end, the reliable message passing encodes messages into reliable packet streams.

The reliable message passing module creates one ring buffer for each remote runtime. When an actor sends a remote actor message, the reliable transmission module allocates a packet, copy the content of the message into the packet and then enqueue the packet into the ring buffer. A message may be splitted into several packets and different messages do not share packets. When the fifth pipeline is scheduled to run, the packets containing remote messages are dequeued from the ring buffer. These packets are configured with a sequential number, appended with a special header to differentiate them from normal data plane packets and sent to their corresponding remote runtimes. The remote runtime sends back acknowledgement packets. Retransmission is fired up in case that the acknowledgement for a packet is not received after a configurable timeout (10 times of the RTT).

We do not use user-level TCP like \cite{mtcp} to implement the reliable message passing module. Because compared with our simple goal of reliably transmitting remote actor messages over an inter-connected L2 network, using a user-level TCP imposes too much processing overhead for reconstructing byte stream into messages. The packet-based reliable message passing provides additional benefits during flow management tasks. For instance, because the second response in the flow migration protocol is sent as a packet on the same path with the dataplane flow packet, it enables us to implement loss-avoidance migration with ease \ref{}. Also, during flow replication, we can directly send the output packet as a message to the replica, without the need to do additional packet copy.

\subsection{Dedicated RPC Thread}
As mentioned in \ref{}, the runtime has a dedicated RPC thread for receiving RPC request sent from the controller. In~\nfactor, the RPC are implemented with GRPC \cite{grpc} and the RPC requests are sent over a reliable TCP connection. To avoid context switches, \nfactor uses a dedicated RPC thread to receive the initial RPC requests and forward these requests to the worker thread through a shared ring buffer. This improves the performance of the worker thread by eliminating potential context switches caused by using kernel networking stack.

\subsection{New Applications}
Asides from NF processing, we build several new applications that is inspired by our light-weight and distributed flow migration. These applications use flow migration to achieve useful practical functionalities, including live NF update, reduce output bandwidth for deduplication NF and ensure reliable and safe MPTCP processing. We evaluate and demonstrate these new applications in our evaluation.

The first application is live NF update. In this application,~\nfactor dynamically updates NF module on a runtime (\ie~update NF modules version, update important NF module configuration files) without interfering active flows running on that runtime.~\nfactor achieves this by dynamically migrating the flows out to a backup runtime, perform update and migrate the flows back. We show in the evaluation \ref{} that the dynamic update ensures no active flows are dropped during the update.

The second application is enable reliable MPTCP \cite{} sub-flow processing. MPTCP sub-flows belonging to the same MPTCP flow may be sent to different runtimes because they have different flow-5-tuple and appear as different flows on the virtual switch. We add a MPTCP sub-flow detection function to each flow actor, such that when the flow actor finishes processing a packet of a MPTCP subflow, it can check which MPTCP flow it belongs to. Then the flow actor performs a consistent hashing using the MPTCP header to select a consistent migration target runtime in the current cluster configuration. Then the flow actor initiates the migration to the migration target actor. This application guarantees that different sub-flows belonging to the same MPTCP flow are always processed by the service chain. This is hard to achieve in existing NFV systems because flows can not initiate active migration request by themselves.

The final application is similar with MPTCP application, which reduces the output bandwidth for deduplication. When the runtime receives a flow, it checks for duplicated content contained in the flow packet. In case it finds out the duplication, the flow actor initiates a migration to migrate itself to a specific runtime. In this way, duplicated flows could be processed on the same runtime, therefore eliminating the output bandwidth on each runtime.
