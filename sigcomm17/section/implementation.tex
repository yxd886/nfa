\section{Implementation}
\label{sec:implementation}

%我们使用C++来实现NFActor。在当前的实现中，NFActor的每一个runtime都跑在一个container中，不同runtime通过BESS进行连接。每一个runtime的端口都是一个BESS的zerocopy vport。

%重用bess module system。
%NFActor runtime的内部架构复用了BESS的module系统以及bess的调度器。我们将bess module system和scheduler代码port到了nfactor runtime的代码里。并且用

\nfactor is implemented in C++ using a combination of DPDK \cite{dpdk}, BESS module system \cite{bess} and GRPC \cite{grpc}. The core functionality of~\nfactor~framework contains around 8500 lines of code.~\nfactor~uses BESS \cite{bess} as the dataplane inter-connection tool to connect different runtimes and virtual switches.~\nfactor uses BESS to build a virtual L2 network inside each server that the coordinator manages, and connect each virtual L2 network to the physical L2 network connecting all the servers. The three ports that are assigned to runtimes are BESS ZeroCopy VPorts, which are high-speed virtual ports implemented with DPDK for raw packet transmission. By connecting the virtual L2 ethernet with the ports of runtimes,~\nfactor connects runtimes running on different server together.

\subsection {Worker Thread}
\label{sec:BessModuleSys}

%The runtime needs to poll packets from the input port, schedule flow actors to run and transmit remote actor messages. To schedule these tasks efficiently, we decide to reuse BESS module systems. BESS module system is specifically designed to schedule packet processing pipelines in high-speed NFV systems, which is a perfect suit to~\nfactor runtime architecture. We port the BESS module system and BESS module scheduler to the runtime and implement all the actor processing tasks as BESS modules. All the implemented BESS modules are  scheduled inside worker thread to achieve flow actor scheduling, packet processing over the service chain and remote actor message transmission.

The worker thread routinely carries out the following four tasks. (i) Poll dataplane packets from intput/output port, run a actor schedule to schedule flow actors to process these packets and sends the packets out from the output/input port after flow actors finishes processing the packets. (ii) Poll packets from control ports, reconstruct packet stream into remote actor messages and send the actor messages to the receiver actors. (The first task also carries out similar processing as the second task because remote messages are also sent to input/output port as shown in Sec.~\ref{sec:resilience}). (iii) Run the liason actor to process RPC requests sent from the coordinator and dispatches flow migration initiation messages to active flow actors in the runtime. (iv) Fetch remote actor messages from a queue that are enqueued by actors during the execution of previous three tasks and send the remote actor messages out from the three ports.

In order to schedule these tasks, we directly port the BESS \cite{bess} module system and BESS module scheduler and implement these tasks as BESS modules. The BESS module scheduler running in the worker thread keeps scheduling these modules in a round-robin fashion. The worker thread are pinned to its own CPU core to decrease the overhead incurred OS scheduling and improve packet processing performance.
%\begin{itemize}

%\item The first/second pipeline polls packets from the input/output port, runs actor scheduler on these packets and sends the packets out from the output/input port.

%\item The third pipeline polls packets from control ports, reconstruct packet stream into remote actor messages and send the actor messages to the receiver actors. (The first/second pipeline also carries out this processing because remote messages are also sent to input/output port \ref{}).

%\item The fourth pipeline schedules coordinator actor to execute RPC requests sent from the controller. In particular, coordinator actor updates the configuration information of other runtimes in the cluster and dispatches flow migration initiation messages to active flow actors in the runtime.

%\item When processing the previous four pipelines, the actors may send remote actor messages. These messages are placed into ring buffers \ref{}. The fifth pipeline fetches remote actor messages from these ring buffers and sends remote actor messages out from corresponding ports.

%\end{itemize}

%The runtime uses BESS scheduler to schedule these 5 pipelines in a round-rubin manner to simulate a time-sharing scheduling.

\subsection{Dedicated RPC Thread}
As mentioned in \ref{sec:overview}, the runtime has a dedicated RPC thread for receiving RPC request sent from the coordinator. In~\nfactor, the RPC are implemented with GRPC \cite{grpc} and the RPC requests are sent over a reliable TCP connection. To avoid context switches, \nfactor uses a dedicated RPC thread to receive the initial RPC requests and forward these requests to the liason actor in the worker thread through a shared ring buffer. This improves the performance of the worker thread by eliminating potential context switches caused by using kernel networking stack.

\subsection{Runtime Container Resource}

\nfactor runs each runtime inside a Docker \cite{docker} container. To use BESS VPort, the container must be configured with the highest system access right (set the --previledge=true argument). Therefore each container actually could uses all the CPU cores on the server. When launching runtimes, the coordinator ensures that the worker threads of the runtimes are pinned to different CPU cores (execept for core 0 and the cores that are used by BESS), as they are busy polling thread and keeps the CPU utilization rate to 100\%. The RPC thread of all the runtimes in the same server are collective pinned to core 0 because they sleep most of the time waiting for RPC requests. The threshold for identifying whether a runtime overloads is set to 100 dropped packet on the input port of the runtime over one second.

\subsection{Customized Actor Library}
\label{sec:actor-library}

To minimize the overhead of actor programming, we choose to implement our own actor library. In this actor library, due to the single-worker-thread design, local actor message transmission is directly implemented as a function call, therefore eliminating the overhead of enqueuing and dequeuing messages from an actor mailbox \cite{actor-wiki}. For remote actor message passing, we assign a unique ID to each runtime and each actor. The sender actor only needs to specify the receiver actor's ID and runtime ID, then the reliable transmission module \ref{sec:ReliableMsgPassing} could deliver the remote actor message to the receiver actor.

To schedule flow actors, we directly run a flow actor scheduler in the first three tasks of the worker thread. The flow actor scheduler redirects both input flow packets and remote actor messages to corresponding flow actors, by looking up the flow actors using the flow-5-tuple from the flow identifier.

Even though the functionality implemented by our customized actor library is very simple compared with other mature actor libraries \cite{akka} \cite{caf}, the simple architecture of our actor library decreases overhead associated with actor processing, enabling~\nfactor to satisfy the high-speed packet processing requirement of mordern NFV system. %Our customized actor library could not achieve complicated ac as other mature actor frameworks do \cite{akka} \cite{caf}. However, due to its simple architecture, it only imposes a small overhead when doing actor processing, therefore it is able to satisfy the high-speed packet processing requirement of modern NFV system.

\subsection{Reliable Message Passing Module}
\label{sec:ReliableMsgPassing}

To reliably deliver remote actor messages, we build a customized reliable message passing module for NFActor framework. Unlike user-level TCP stack, where messages are inserted into a reliable byte stream and transmitted to the other end, the reliable message passing encodes messages into reliable packet streams.

The reliable message passing module creates one ring buffer for each remote runtime and each virtual switch. When an actor sends a remote actor message, the reliable transmission module allocates a packet, copy the content of the message into the packet and then enqueue the packet into the ring buffer. A message may be splitted into several packets and different messages do not share packets. When the fourth task of the worker thread is scheduled to run, the packets containing remote messages are dequeued from the ring buffer. These packets are configured with a sequential number, appended with a special header to differentiate them from normal data plane packets and sent to their corresponding remote runtimes. The remote runtime sends back acknowledgement packets. Retransmission is fired up in case that the acknowledgement for a packet is not received after a configurable timeout (10 times of the RTT).

We do not use user-level TCP like \cite{mtcp} to implement the reliable message passing module. Because compared with our simple goal of reliably transmitting remote actor messages over an inter-connected L2 network, using a user-level TCP imposes too much processing overhead for reconstructing byte stream into messages. The packet-based reliable message passing provides additional benefits during resilience operation. For instance, because the second response in the flow migration protocol is sent as a packet on the same path with the dataplane flow packet, it enables us to implement loss-avoidance migration with ease (Sec.~\ref{sec:resilience}). Also, during flow replication, we can directly send the output packet as a message to the replica, without the need to do additional packet copy.

%I'm about to move it to evaluation secition.
\subsection{New Applications}

Asides from NF processing, we build several new applications that is inspired by our light-weight and distributed flow migration. These applications use flow migration to achieve useful practical functionalities, including live NF update, reduce output bandwidth for deduplication NF and ensure reliable and safe MPTCP processing. We evaluate and demonstrate these new applications in our evaluation.

The first application is live NF update. In this application,~\nfactor dynamically updates NF module on a runtime (\ie~update NF modules version, update important NF module configuration files) without interfering active flows running on that runtime.~\nfactor achieves this by dynamically migrating the flows out to a backup runtime, perform update and migrate the flows back. We show in the evaluation \ref{} that the dynamic update ensures no active flows are dropped during the update.

The second application is enable reliable MPTCP \cite{} sub-flow processing. MPTCP sub-flows belonging to the same MPTCP flow may be sent to different runtimes because they have different flow-5-tuple and appear as different flows on the virtual switch. We add a MPTCP sub-flow detection function to each flow actor, such that when the flow actor finishes processing a packet of a MPTCP subflow, it can check which MPTCP flow it belongs to. Then the flow actor performs a consistent hashing using the MPTCP header to select a consistent migration target runtime in the current cluster configuration. Then the flow actor initiates the migration to the migration target actor. This application guarantees that different sub-flows belonging to the same MPTCP flow are always processed by the service chain. This is hard to achieve in existing NFV systems because flows can not initiate active migration request by themselves.

The final application is similar with MPTCP application, which reduces the output bandwidth for deduplication. When the runtime receives a flow, it checks for duplicated content contained in the flow packet. In case it finds out the duplication, the flow actor initiates a migration to migrate itself to a specific runtime. In this way, duplicated flows could be processed on the same runtime, therefore eliminating the output bandwidth on each runtime.
